{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "e30af5b2f7f84a6d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading the data\n",
    "I load the MNIST dataset and normalize the pixel values to be between -1 and 1 because I am going to use Tanh as an activation function. Then I split the dataset to train and test sets."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1594890fc2288bc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "mnist_train = datasets.MNIST(root = './data', train = True, download = True, transform = transform)\n",
    "train_loader = torch.utils.data.DataLoader(mnist_train, batch_size = 100, shuffle = True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "f65bcd6a96b8e755"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generator\n",
    "I rewrite the architecture described in the paper in PyTorch (“We trained a multilayer perceptron with 100 input units, two hidden layers of 1200 ReLU units each, and an output layer with 784 sigmoid units.”), where I initialize the pass by creating a noise vector. I use Tanh function instead of Sigmoid for stability (inspired by - __[link to the blog post on towardsdatascience](https://towardsdatascience.com/batch-norm-explained-visually-how-it-works-and-why-neural-networks-need-it-b18919692739/)__ )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30672574d052cb96"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim=100):\n",
    "        super(Generator, self).__init__()\n",
    "        self.hidden1 = nn.Linear(noise_dim, 1200)\n",
    "        self.hidden2 = nn.Linear(1200, 1200)\n",
    "        self.output = nn.Linear(1200, 784)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.relu(self.hidden1(z))\n",
    "        x = self.relu(self.hidden2(x))\n",
    "        x = self.tanh(self.output(x))\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "d78bc4eba44fb0be"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Discriminator\n",
    "For the discriminator I also rewrite te architecture from the architecture, with a slight change - LeakyReLU instead of maxout to avoid the vanishing gradient problem."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9aa33e8a56b2a1a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.hidden1 = nn.Linear(784, 240)\n",
    "        self.hidden2 = nn.Linear(240, 240)\n",
    "        self.output = nn.Linear(240, 1)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.leaky_relu(self.hidden1(x))\n",
    "        x = self.leaky_relu(self.hidden2(x))\n",
    "        x = self.sigmoid(self.output(x))\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "275c10ff99359815"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initialization\n",
    "Preparing the Discriminator and Generator for training, choosing Binary Cross Entropy Loss (as the task of training is condensed to predicting between real and fake labels) and choosing Stochastic Gradient Descent as in the paper."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1522d6296ded6f3c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "G = Generator(noise_dim=100).to(device)\n",
    "D = Discriminator().to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimD = optim.SGD(D.parameters(), lr = 0.1, momentum = 0.5)\n",
    "optimG = optim.SGD(G.parameters(), lr = 0.1, momentum = 0.5)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "5210f74107c3c6f9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training Loop\n",
    "Resembling the minmax game in the paper at each training step, we are taking real image from the dataset passing it through the discriminator with the goal of label 1. Next we generate a fake image through the generator and give it to the discriminator with the goal of label 0. Then we adjust the weights accordingly. For the generator we pass the noise and see if generated image can be distinguished from fake by the discriminator, and adjust the weights based on how well we full the discriminator. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21034696e74221eb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "real_label = 1.0\n",
    "fake_label = 0.0\n",
    "num_epochs = 50\n",
    "batch_size = 100\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    for batch_idx, (real_images, _) in enumerate(train_loader):\n",
    "        real_images = real_images.to(device)\n",
    "\n",
    "        #discriminator training\n",
    "        optimD.zero_grad()\n",
    "        real_images_flat = real_images.view(real_images.size(0), -1)\n",
    "        output_real = D(real_images_flat)\n",
    "        labels_real = torch.full((real_images.size(0), 1), real_label, device=device)\n",
    "        loss_real = criterion(output_real, labels_real)\n",
    "\n",
    "        noise = torch.randn(real_images.size(0), 100, device=device)\n",
    "        fake_images = G(noise).detach()\n",
    "        output_fake = D(fake_images)\n",
    "        labels_fake = torch.full((real_images.size(0), 1), fake_label, device=device)\n",
    "        loss_fake = criterion(output_fake, labels_fake)\n",
    "\n",
    "        lossD = (loss_real + loss_fake) / 2\n",
    "        lossD.backward()\n",
    "        optimD.step()\n",
    "\n",
    "        #generator training\n",
    "        optimG.zero_grad()\n",
    "        noise = torch.randn(real_images.size(0), 100, device=device)\n",
    "        fake_images = G(noise)\n",
    "        output_fake_for_G = D(fake_images)\n",
    "        labels_for_G = torch.full((real_images.size(0), 1), real_label, device=device)\n",
    "        lossG = criterion(output_fake_for_G, labels_for_G)\n",
    "        lossG.backward()\n",
    "        optimG.step()\n",
    "\n",
    "    print(f\"Epoch {epoch}: D_loss={lossD.item():.4f}, G_loss={lossG.item():.4f}\")\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        z = torch.randn(16, 100, device=device)\n",
    "        fake_images = G(z)\n",
    "        fake_images = fake_images.view(-1, 28, 28).cpu().detach()\n",
    "\n",
    "        fig, axes = plt.subplots(4, 4, figsize=(6,6))\n",
    "        for i, ax in enumerate(axes.flat):\n",
    "            ax.imshow(fake_images[i], cmap='gray')\n",
    "            ax.axis('off')\n",
    "        plt.suptitle(f\"Generated Images after Epoch {epoch}\", fontsize=14)\n",
    "        plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "128a59a339bbad66"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
